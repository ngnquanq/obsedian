**Cross Entropy Loss** là một thước đo để đánh giá một mô hình phân loại làm tốt đến mức nào. 
- Cụ thể hơn, hàm cross-entropy loss đo lường sự khác biệt giữa phân phối xác suất của dự báo so với phân phối thật sự của nhãn. 
- Đối với các bài toán **phân loại nhị phân** thì giá trị của hàm loss **rơi vào khoảng từ 0 đến 1** . Trong trường hợp này, nếu **loss = 0 thì mô hình phân loại chính xác 100%**. 
- Các trường hợp phân loại khác, giá trị Loss có thể nằm ngoài khoảng 0,1 như ở trên đề cập
- Thường được sử dụng luân phiên với `logistic loss` hoặc là `log loss` hoặc là `binary cross entropy` (cho trường hợp phân loại nhị phân). 
## Công thức

Công thức toán đằng sau hàm này hiểu đơn giản là chúng ta lấy: **tích giữa phân phối thật và log(phân phối dự đoán), và lấy trên toàn bộ nhãn trong phân phối**

$$
H(P^*|P) = -\sum_{i}P^*(i)\times\log(P(i))
$$

Với i trong trường hợp này là các nhãn của chúng ta, ví dụ như bài toán phân loại của mình có 2 nhãn thì $i$ sẽ bằng 2, kiểu kiểu vậy. 

$P^*(i)$ là phân phối thật sự của nhãn, $P(i)$ là phân phối của nhãn dự đoán
## Cơ sở lý thuyết đằng sau hàm loss cross entropy

### Thảo luận về đầu ra.

Có bao giờ mọi người phân vân rằng, tại sao khi thiết kế mô hình cho một bài toán phân loại (lấy phân loại đa nhãn cho nó chung nhất). Người ta lại thiết kế sao cho số node của layer cuối cùng nó bằng với số class của người ta muốn phân loại chưa?

Lấy ví dụ: Một **bài toán phân loại** với một **mô hình với bộ tham số $\theta$ cho trước**. Mục tiêu của mô hình phân loại này là phân loại ảnh chó mèo, hay là con gì gì đấy, ví dụ như có 10 nhãn. Lý thuyết xác suất ([[Probability Theory]]) tranh luận rằng output tự nhiên nhất chính là toàn bộ phân phối xác suất. 

Lí do cho việc này đó là có một chút ambiguity, cụ thể hơn, giờ ví dụ bạn thả vô một hình con mèo (trong hình chỉ có một con mèo thôi), một mô hình tốt sẽ nói bạn biết đó là con mèo, nhưng nó sẽ không thể chắc chắn được đó là con mèo, mà nó sẽ rào lại (giới hạn của nó là số nhãn), cụ thể hơn thì các mô hình phân loại **không đưa ra kết quả tuyệt đối** mà nó sẽ **có sự không chắc chắn khi phân loại nhãn thật của một điểm dữ liệu (ảnh trong trường hợp này)**. Và phân phối nhãn này capture được toàn bộ sự không chắc chắn này bằng cách cung cấp thông tin xác suất cho mỗi class khác nhau. Về cơ bản cách thiết kế như vậy phản ánh được sự tự tin của mô hình trong việc gán nhãn.  

Một lí do khác là **làm giàu thông tin**. Việc dự đoán chỉ duy nhất một con số thường không mang lại nhiều ý nghĩa thông tin, lấy ví dụ như trong tấm ảnh không phải con mèo nữa, mà là cả chó cả mèo, thì model nếu dự đoán chỉ ra 1 con số, nó sẽ dự đoán gì? Do đó việc thể hiện kết quả dưới dạng phân phối xác suất cung cấp một cái nhìn toàn diện hơn, thay vì mô hình trả ra một chữ "chó" thì nó có thể trả ra một vector $p = (.6 ,.4)$ với $.6$ là xác suất của data point đó là chó . Kiểu kiểu vậy, như vậy thì thể hiện output dưới dạng một phân phối xác suất sẽ làm giàu thông tin hơn. 

### Mục tiêu
Khi mà thiết kế mô hình, huấn luyện mô hình, chúng ta phải đặt ra cái chuẩn chung (xem lại [[0. Why Loss function]] để biết thêm chi tiết). 

Ở đây, một mô hình tốt sẽ đạt được điều kiện sau:
$$
P(y|x_i; \theta) = P^*(y|x_i) 
$$Và điều kiện này sẽ thỏa mãn nếu như $\theta$ là một bộ tham số phù hợp. 

### Các mô hình Deep Learing. 
Như ở trên có nói, đó cũng chính là lý do tại sao mà **hầu như trong mọi mô hình phân loại, đều sẽ có một lớp `softmax` ở cuối, và sử dụng `onehot encoding` để encode các nhãn**. Lưu ý là `PyTorch` cài sẵn `softmax` vào hàm loss `nn.CELoss()` rồi nên thường các mô hình sẽ không có layer `softmax` ở cuối nữa. 

1. Hàm `softmax` cho phép đưa giá trị output về một phân phối xác suất thật sự (tổng các xác suất = 1 và không có xác suất nào là số âm). 
2. Onehot encoding đưa các nhãn thật về một hard distribution (chưa biết dịch sao - nói chung là xác suất của nhãn thật bằng 1, còn của các nhãn khác bằng 0)


## Source tham khảo
1. https://www.youtube.com/watch?v=Pwgpl9mKars
2. 